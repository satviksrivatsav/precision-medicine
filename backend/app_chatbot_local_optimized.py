# File: backend/app_chatbot_smart.py

import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os
import re
import time
from datetime import datetime
import hashlib

# --- Page Configuration ---
st.set_page_config(page_title="Smart Q&A Bot", page_icon="ğŸ¤–")
st.title("ğŸ¤– Smart Q&A Bot")

# --- Smart Response System ---
class SmartChatbot:
    def __init__(self):
        self.greetings = {
            'hi': "Hi there! ğŸ‘‹ I'm here to help you with health-related questions. What would you like to know?",
            'hello': "Hello! ğŸ˜Š I'm your health assistant. Feel free to ask me anything about health topics!",
            'hey': "Hey! ğŸ‘‹ How can I help you today?",
            'good morning': "Good morning! â˜€ï¸ Hope you're having a great day. What can I help you with?",
            'good afternoon': "Good afternoon! ğŸŒ¤ï¸ How can I assist you today?",
            'good evening': "Good evening! ğŸŒ™ What would you like to know?",
            'how are you': "I'm doing great, thanks for asking! ğŸ˜Š I'm here and ready to help with your health questions.",
            'what is your name': "I'm your friendly health assistant chatbot! ğŸ¤– You can ask me questions about various health topics.",
            'who are you': "I'm a health-focused chatbot designed to help answer your medical and health-related questions! ğŸ¥"
        }
        
        self.thanks_responses = [
            "You're welcome! ğŸ˜Š Happy to help!",
            "Glad I could help! ğŸ‘ Feel free to ask more questions.",
            "You're very welcome! ğŸ™‚ Anything else you'd like to know?",
            "My pleasure! ğŸ˜Š I'm here whenever you need help."
        ]
        
        self.unknown_responses = [
            "I don't have information about that topic in my knowledge base yet. ğŸ¤” Try asking about health-related topics!",
            "I'm not sure about that one! ğŸ˜… I specialize in health and medical questions. Got any health queries?",
            "That's not in my knowledge base currently. ğŸ“š I'm great with health and medical questions though!",
            "I don't know about that topic yet! ğŸ¤·â€â™€ï¸ But I'm here to help with health-related questions!"
        ]
        
        # Minimum similarity threshold for valid responses
        self.similarity_threshold = 0.5
        
    def is_greeting(self, text):
        """Check if the input is a greeting"""
        text_lower = text.lower().strip()
        for greeting in self.greetings.keys():
            if greeting in text_lower:
                return greeting
        return None
    
    def is_thanks(self, text):
        """Check if the input is a thank you"""
        thanks_words = ['thank', 'thanks', 'appreciate', 'grateful']
        text_lower = text.lower()
        return any(word in text_lower for word in thanks_words)
    
    def is_goodbye(self, text):
        """Check if the input is a goodbye"""
        goodbye_words = ['bye', 'goodbye', 'see you', 'take care', 'farewell']
        text_lower = text.lower()
        return any(word in text_lower for word in goodbye_words)
    
    def get_response_type(self, text, retrieved_nodes):
        """Determine the type of response needed"""
        # Check for greetings first
        greeting = self.is_greeting(text)
        if greeting:
            return 'greeting', greeting
        
        # Check for thanks
        if self.is_thanks(text):
            return 'thanks', None
        
        # Check for goodbye
        if self.is_goodbye(text):
            return 'goodbye', None
        
        # Check if we have good retrieval results
        if retrieved_nodes and len(retrieved_nodes) > 0:
            best_score = retrieved_nodes[0].score
            if best_score >= self.similarity_threshold:
                return 'knowledge', None
            else:
                return 'unknown', None
        
        return 'unknown', None

# Initialize smart chatbot
smart_bot = SmartChatbot()

# --- Caching System ---
@st.cache_data(ttl=3600)  # Cache for 1 hour
def get_cached_response(question_hash, question):
    """Cache responses to avoid recomputation"""
    return None  # This will be populated by actual responses

def generate_question_hash(question):
    """Generate a hash for the question for caching"""
    return hashlib.md5(question.lower().strip().encode()).hexdigest()

# Custom parser for Q&A format (same as before)
def parse_qa_file(file_path):
    """Parse Q&A formatted files into separate documents"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    qa_pairs = re.split(r'\n\s*Q:', content)
    documents = []
    
    for pair in qa_pairs:
        if not pair.strip():
            continue
            
        if not pair.startswith('Q:'):
            pair = 'Q:' + pair
            
        lines = pair.strip().split('\n')
        if len(lines) >= 2:
            question = lines[0].replace('Q:', '').strip()
            answer_lines = []
            
            for line in lines[1:]:
                if line.startswith('A:'):
                    answer_lines.append(line.replace('A:', '').strip())
                elif line.strip() and not line.startswith('Q:'):
                    answer_lines.append(line.strip())
                else:
                    break
            
            if question and answer_lines:
                answer = ' '.join(answer_lines)
                doc_text = f"Question: {question}\nAnswer: {answer}"
                doc = Document(
                    text=doc_text,
                    metadata={
                        'question': question,
                        'answer': answer,
                        'file_name': os.path.basename(file_path)
                    }
                )
                documents.append(doc)
    
    return documents

@st.cache_resource
def setup_pipeline():
    """Setup the knowledge base pipeline with caching"""
    Settings.embed_model = HuggingFaceEmbedding(model_name="all-MiniLM-L6-v2")
    Settings.llm = None
    Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    
    try:
        context_dir = "./knowledgeBase"
        if not os.path.exists(context_dir):
            st.error(f"The '{context_dir}' directory is missing.")
            return None

        all_documents = []
        txt_files = [f for f in os.listdir(context_dir) if f.endswith('.txt')]
        
        if not txt_files:
            st.error(f"No .txt files found in '{context_dir}'.")
            return None
            
        for filename in txt_files:
            file_path = os.path.join(context_dir, filename)
            try:
                qa_docs = parse_qa_file(file_path)
                if qa_docs:
                    all_documents.extend(qa_docs)
                else:
                    reader = SimpleDirectoryReader(input_files=[file_path])
                    docs = reader.load_data()
                    all_documents.extend(docs)
            except Exception as e:
                st.warning(f"Could not load {filename}: {e}")
        
        if not all_documents:
            st.error("No documents were loaded successfully.")
            return None

        with st.spinner(f"Building smart knowledge base from {len(all_documents)} documents..."):
            index = VectorStoreIndex.from_documents(all_documents)
        
        retriever = index.as_retriever(
            similarity_top_k=3,  # Get more options for better filtering
            verbose=False
        )
        
        return retriever, len(all_documents)
        
    except Exception as e:
        st.error(f"Failed to build the index. Error: {e}")
        return None, 0

# Initialize pipeline
result = setup_pipeline()
if result:
    retriever, doc_count = result
else:
    retriever, doc_count = None, 0

# --- Session State for Conversation History ---
if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []

if 'response_cache' not in st.session_state:
    st.session_state.response_cache = {}

st.divider()

# --- UI for Smart Chatbot ---
if retriever:
    st.success(f"âœ… Smart knowledge base ready with {doc_count} documents")
    
    # Show conversation history
    if st.session_state.conversation_history:
        st.subheader("ğŸ’¬ Conversation")
        for i, (user_msg, bot_msg, timestamp) in enumerate(st.session_state.conversation_history[-5:]):  # Show last 5
            with st.container():
                st.write(f"**You ({timestamp}):** {user_msg}")
                st.write(f"**Bot:** {bot_msg}")
                st.write("---")
    
    st.header("Ask me anything!")
    
    user_question = st.text_input("Type your message here:", key="user_input")

    if user_question:
        start_time = time.time()
        
        # Generate hash for caching
        question_hash = generate_question_hash(user_question)
        
        # Check cache first
        if question_hash in st.session_state.response_cache:
            response_text = st.session_state.response_cache[question_hash]
            st.info("ğŸš€ Retrieved from cache (super fast!)")
        else:
            with st.spinner("ğŸ¤– Thinking..."):
                try:
                    # Get retrieval results
                    retrieved_nodes = retriever.retrieve(user_question)
                    
                    # Determine response type
                    response_type, greeting_key = smart_bot.get_response_type(user_question, retrieved_nodes)
                    
                    if response_type == 'greeting':
                        response_text = smart_bot.greetings[greeting_key]
                    elif response_type == 'thanks':
                        response_text = smart_bot.thanks_responses[hash(user_question) % len(smart_bot.thanks_responses)]
                    elif response_type == 'goodbye':
                        response_text = "Goodbye! ğŸ‘‹ Take care and feel free to come back anytime you have health questions!"
                    elif response_type == 'knowledge' and retrieved_nodes:
                        # Extract the best answer
                        best_node = retrieved_nodes[0]
                        text = best_node.get_text()
                        
                        if "Answer:" in text:
                            response_text = text.split("Answer:", 1)[1].strip()
                        else:
                            response_text = text
                        
                        # Add confidence indicator
                        confidence = best_node.score
                        if confidence > 0.8:
                            response_text += "\n\nâœ… High confidence answer"
                        elif confidence > 0.6:
                            response_text += "\n\nâš ï¸ Medium confidence answer"
                    else:
                        # Unknown topic
                        response_text = smart_bot.unknown_responses[hash(user_question) % len(smart_bot.unknown_responses)]
                    
                    # Cache the response
                    st.session_state.response_cache[question_hash] = response_text
                    
                except Exception as e:
                    response_text = f"Sorry, I encountered an error: {str(e)} ğŸ˜…"
        
        # Display response
        st.subheader("ğŸ¤– Response:")
        st.success(response_text)
        
        # Show processing time
        processing_time = time.time() - start_time
        st.caption(f"âš¡ Response time: {processing_time:.2f} seconds")
        
        # Add to conversation history
        timestamp = datetime.now().strftime("%H:%M")
        st.session_state.conversation_history.append((user_question, response_text, timestamp))
        
        # Keep only last 10 conversations to save memory
        if len(st.session_state.conversation_history) > 10:
            st.session_state.conversation_history = st.session_state.conversation_history[-10:]
        
        # Show additional info for knowledge-based responses
        if 'retrieved_nodes' in locals() and retrieved_nodes and len(retrieved_nodes) > 0:
            if retrieved_nodes[0].score >= smart_bot.similarity_threshold:
                with st.expander("ğŸ“š Source Information"):
                    best_node = retrieved_nodes[0]
                    st.write(f"**Source:** {best_node.metadata.get('file_name', 'N/A')}")
                    st.write(f"**Confidence Score:** {best_node.score:.3f}")
                    if 'question' in best_node.metadata:
                        st.write(f"**Matched Question:** {best_node.metadata['question']}")

else:
    st.error("âŒ Smart knowledge base could not be initialized.")

# --- Sidebar with Cache Stats ---
with st.sidebar:
    st.header("ğŸš€ Performance Stats")
    st.metric("Cached Responses", len(st.session_state.response_cache))
    st.metric("Conversation History", len(st.session_state.conversation_history))
    
    if st.button("ğŸ—‘ï¸ Clear Cache"):
        st.session_state.response_cache.clear()
        st.session_state.conversation_history.clear()
        st.success("Cache cleared!")
        st.rerun()
    
    st.header("ğŸ’¡ Tips")
    st.info("Try saying:\n- Hi, Hello\n- Thank you\n- Health questions\n- Goodbye")
    
    st.header("âš¡ Speed Optimizations")
    st.success("âœ… Response caching\nâœ… Conversation memory\nâœ… Smart filtering\nâœ… Lightweight embeddings")